{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Pipeline \n",
    "This pipeline is used to create features for our textual data\n",
    "1. Data Collection\n",
    "2. Tokenization, stopword removal, stemming\n",
    "3. Building a common vocabulary\n",
    "4. Vectorizing the documents\n",
    "5. Performing classification/clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "# https://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.util.LazyCorpusLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(brown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CategorizedTaggedCorpusReader in '/Users/riagupta/nltk_data/corpora/brown'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown?\n",
    "\n",
    "# A reader for part-of-speech tagged corpora whose documents are divided into categories \n",
    "# based on their file identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='fiction')\n",
    "# Returns a list of sentences, each encoded as a list of word strings. We can specify category (or leave blank for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4249"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "\n",
    "# ie, 4249 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Thirty-three'], ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments', '.']\n"
     ]
    }
   ],
   "source": [
    "print(data[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization, Stopword Removal and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Tokenization\n",
    "Converting each sentence into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a very pleasant day, the weather was cool and there were light showers. I went to the market to buy some fruits.\n"
     ]
    }
   ],
   "source": [
    "text = \"It was a very pleasant day, the weather was cool and there were light showers. I went to the market to buy some fruits.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['It was a very pleasant day, the weather was cool and there were light showers.', 'I went to the market to buy some fruits.']\n"
     ]
    }
   ],
   "source": [
    "# To split our text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(type(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['it', 'was', 'a', 'very', 'pleasant', 'day', ',', 'the', 'weather', 'was', 'cool', 'and', 'there', 'were', 'light', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "# To split our text into words\n",
    "words = word_tokenize(sentences[0].lower())\n",
    "print(type(words))\n",
    "print(words)\n",
    "# Convert to lower case to perform proper stopword removal later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing using Regular Expressions\n",
    "Word tokenizer cant handle complex tokenizations. So we use RegExp Tokenizer class in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send all the 50 documents related to clauses 1,2,3 at abc@gmail.com']\n",
      "['send', 'all', 'the', '50', 'documents', 'related', 'to', 'clauses', '1,2,3', 'at', 'abc', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Send all the 50 documents related to clauses 1,2,3 at abc@gmail.com\"\n",
    "\n",
    "sents = sent_tokenize(text2)\n",
    "print(sents)\n",
    "\n",
    "word_list = word_tokenize(sents[0].lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z@]+\")\n",
    "# Pass a regular expression pattern to remove whatever you want to remove\n",
    "# This regular expression allows chars a-z, A-Z, and spcl char @ => So, it only removes numbers\n",
    "# Create a regular expression using cheat sheet on regexpal.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send all the 50 documents related to clauses 1,2,3 at abc@gmail.com\n",
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'clauses', 'at', 'abc@gmail', 'com']\n"
     ]
    }
   ],
   "source": [
    "print(text2)\n",
    "print(tokenizer.tokenize(text2))     # With numbers removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Stopword Removal\n",
    "Removing unimportant words like is, an, the, it, there, and, has, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(sw))\n",
    "print(type(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(text):\n",
    "    useful_words = [w for w in text if w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasant', 'day', ',', 'weather', 'cool', 'light', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "# Filtering words from our sentence using list comprehension\n",
    "\n",
    "useful_words = [w for w in words if w not in sw]\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Stemming\n",
    "- A process that transforms words to (verbs, plurals) to their radical form\n",
    "- Reducing words to their root words.\n",
    "- Purpose: Preserve the semantics of the sentence without increasing the number of unique tokens\n",
    "\n",
    "eg. \n",
    "- Cat jumps\n",
    "- Cat jumped\n",
    "- Cat is jumping \n",
    "\n",
    "All can be reduced to jump <br>\n",
    "This can also be done through lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foxes', 'love', 'to', 'make', 'jumps', 'the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lovely', 'dog', 'from', 'a', 'feet', 'high', 'wall']\n"
     ]
    }
   ],
   "source": [
    "text = \"Foxes love to make jumps. The quick brown fox was seen jumping over the lovely dog from a 6 feet high wall\"\n",
    "\n",
    "words_list = tokenizer.tokenize(text.lower())\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foxes', 'love', 'make', 'jumps', 'quick', 'brown', 'fox', 'seen', 'jumping', 'lovely', 'dog', 'feet', 'high', 'wall']\n"
     ]
    }
   ],
   "source": [
    "words_list = filter_words(words_list)\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Stemmers:\n",
    "1. Snowball Stemmer (Multilingual => supports French, German, Russian, etc)\n",
    "2. Porter Stemmer\n",
    "3. Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jump\n",
      "jump\n",
      "jumper\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('jumped'))\n",
    "print(ps.stem('jumps'))\n",
    "print(ps.stem('jumping'))\n",
    "print(ps.stem('jumper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "awesom\n",
      "teeth\n",
      "teenag\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('lovely'))\n",
    "print(ps.stem('awesome'))\n",
    "print(ps.stem('teeth'))\n",
    "print(ps.stem('teenager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jump\n",
      "jump\n",
      "jump\n",
      "lov\n",
      "awesom\n",
      "tee\n",
      "teen\n"
     ]
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "\n",
    "print(ls.stem('jumped'))\n",
    "print(ls.stem('jumps'))\n",
    "print(ls.stem('jumping'))\n",
    "print(ls.stem('jumper'))\n",
    "print(ls.stem('lovely'))\n",
    "print(ls.stem('awesome'))\n",
    "print(ls.stem('teeth'))\n",
    "print(ls.stem('teenager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jump\n",
      "jump\n",
      "jumper\n",
      "love\n",
      "awesom\n",
      "teeth\n",
      "teenag\n"
     ]
    }
   ],
   "source": [
    "ss = SnowballStemmer('english')\n",
    "print(ss.stem('jumped'))\n",
    "print(ss.stem('jumps'))\n",
    "print(ss.stem('jumping'))\n",
    "print(ss.stem('jumper'))\n",
    "print(ss.stem('lovely'))\n",
    "print(ss.stem('awesome'))\n",
    "print(ss.stem('teeth'))\n",
    "print(ss.stem('teenager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parl\n",
      "parl\n",
      "parl\n",
      "parl\n",
      "parlon\n",
      "fin\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "ss2 = SnowballStemmer('french')\n",
    "\n",
    "print(ss2.stem(\"parlais\"))\n",
    "print(ss2.stem(\"parles\"))\n",
    "print(ss2.stem(\"parler\"))\n",
    "print(ss2.stem(\"parlerai\"))\n",
    "print(ss2.stem(\"parlons\"))\n",
    "print(ss2.stem(\"finir\"))\n",
    "print(ss2.stem(\"finit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to perform all 3 steps: Tokenization, stopword removal, and stemming, and also removes any leading\n",
    "# or trailing white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He had nothing much to say to her but that he said anything seemed to please her and he accompanied her on some of her unusually searching tours of Tokyo .\n"
     ]
    }
   ],
   "source": [
    "text = brown.sents(categories='romance')\n",
    "sent = ' '.join(text[3743])\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           He had nothing much to say to her but that          he said anything seemed to please her and he accompanied her on some of her unusually searching tours of Tokyo.             \n",
      "\n",
      "['he', 'had', 'nothing', 'much', 'to', 'say', 'to', 'her', 'but', 'that', 'he', 'said', 'anything', 'seemed', 'to', 'please', 'her', 'and', 'he', 'accompanied', 'her', 'on', 'some', 'of', 'her', 'unusually', 'searching', 'tours', 'of', 'tokyo']\n",
      "['nothing', 'much', 'say', 'said', 'anything', 'seemed', 'please', 'accompanied', 'unusually', 'searching', 'tours', 'tokyo']\n",
      "\n",
      "['nothing', 'much', 'say', 'said', 'anything', 'seemed', 'please', 'accompanied', 'unusually', 'searching', 'tours', 'tokyo']\n",
      "\n",
      "['noth', 'much', 'say', 'said', 'anyth', 'seem', 'pleas', 'accompani', 'unusu', 'search', 'tour', 'tokyo']\n",
      "\n",
      "['noth', 'much', 'say', 'said', 'anyth', 'seem', 'pleas', 'accompani', 'unusu', 'search', 'tour', 'tokyo']\n",
      "\n",
      "['noth', 'much', 'say', 'said', 'anyth', 'seem', 'pleas', 'accompany', 'unus', 'search', 'tour', 'tokyo']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cleanup(sent):\n",
    "    print(sent+'\\n')\n",
    "    tokenizer = RegexpTokenizer(\"[a-zA-Z]+\")\n",
    "    sent = tokenizer.tokenize(sent.lower())\n",
    "    print(sent)\n",
    "    \n",
    "    sw = stopwords.words('english')\n",
    "    sent = [w for w in sent if w not in sw]\n",
    "    print(sent)\n",
    "    \n",
    "    ss = SnowballStemmer('english')\n",
    "    ps = PorterStemmer()\n",
    "    ls = LancasterStemmer()\n",
    "    stemmed_words = []\n",
    "    stemmed_words2 = []\n",
    "    stemmed_words3 = []\n",
    "\n",
    "    for w in sent:\n",
    "        stemmed_words.append(ss.stem(w))\n",
    "        stemmed_words2.append(ps.stem(w))\n",
    "        stemmed_words3.append(ls.stem(w))\n",
    "\n",
    "    print()\n",
    "    print(sent)\n",
    "    print()\n",
    "    print(stemmed_words)\n",
    "    print()\n",
    "    print(stemmed_words2)\n",
    "    print()\n",
    "    print(stemmed_words3)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "text = \"           He had nothing much to say to her but that          he said anything seemed to please her and he accompanied her on some of her unusually searching tours of Tokyo.             \"\n",
    "   \n",
    "cleanup(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Lemmatization (Try it yourself)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "l = WordNetLemmatizer()\n",
    "l.lemmatize(\"crying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building Common Vocabulary\n",
    "After extracting all the important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vectorized_corpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 42)\n",
      "[[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1\n",
      "  0 2 0 1 0 2]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 3 0 0 0\n",
      "  0 0 0 0 1 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
      "  0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus.shape)\n",
    "print(vectorized_corpus)\n",
    "\n",
    "# Each vector entry denotes the frequency of the word at that index in the dictionary. Indices are described in random\n",
    "# order and can be checked in next cell using get_feature_names() function\n",
    "\n",
    "# Eg. word \"world\" is at last index in dictionary and appears 2x in sentence 1 => array has value 2 at first row \n",
    "# last index\n",
    "# Eg. word \"the\" is at 10th last index in dictionary and appears 3x in sentence 3 => array has value 3 at third row \n",
    "# 10th last index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'at', 'based', 'be', 'capt', 'confident', 'cricket', 'cup', 'elections', 'exciting', 'hearts', 'held', 'indian', 'is', 'kohli', 'lanka', 'laurate', 'lok', 'movie', 'next', 'nobel', 'of', 'people', 'pm', 'raazi', 'real', 'sabha', 'says', 'spy', 'sri', 'story', 'team', 'the', 'thriller', 'upon', 'virat', 'we', 'will', 'win', 'wins', 'won', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'wins': 39, 'world': 41, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'win': 38, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 40, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary which maps word to index\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "[array(['an', 'at', 'based', 'cup', 'elections', 'exciting', 'hearts',\n",
      "       'held', 'indian', 'is', 'kohli', 'lanka', 'laurate', 'lok',\n",
      "       'movie', 'next', 'nobel', 'of', 'people', 'pm', 'raazi', 'real',\n",
      "       'sabha', 'says', 'spy', 'sri', 'story', 'team', 'the', 'thriller',\n",
      "       'upon', 'virat', 'we', 'will', 'win', 'wins', 'won', 'world'],\n",
      "      dtype='<U9')]\n",
      "[array(['an', 'at', 'based', 'be', 'capt', 'confident', 'cricket', 'cup',\n",
      "       'elections', 'exciting', 'hearts', 'held', 'indian', 'is', 'kohli',\n",
      "       'lanka', 'laurate', 'lok', 'movie', 'next', 'nobel', 'of',\n",
      "       'people', 'pm', 'raazi', 'real', 'sabha', 'says', 'spy', 'sri',\n",
      "       'story', 'team', 'the', 'thriller', 'upon', 'virat', 'we', 'will',\n",
      "       'win', 'wins', 'won', 'world'], dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "# Reverse mapping: Given any self-created vector, we want to see what sentence is created\n",
    "import numpy as np\n",
    "\n",
    "vec = np.ones((42,))\n",
    "vec[3:7] = 0\n",
    "print(vec)\n",
    "print()\n",
    "\n",
    "print(cv.inverse_transform(vec))\n",
    "vec[3:7] = 1\n",
    "\n",
    "print(cv.inverse_transform(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Check index of any word\n",
    "print(cv.vocabulary_['virat'])\n",
    "print(cv.vocabulary_['kohli'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vectorize the Document\n",
    "Use the bag-of-words model to convert this vocabulary to vectors. \n",
    "\n",
    "eg. It was raining and the cat jumped <br>\n",
    "Vocab => rain, cat, jump\n",
    "\n",
    "For sentence: The cat was jumping, vector is [0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a corpus with one million words, each sentence vector is represented as a vector of size 1 million, so,\n",
    "# Effectively reduce the size of the vector\n",
    "\n",
    "def myTokenizer(sentence):\n",
    "    words = tokenizer.tokenize(sentence)\n",
    "    return filter_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Classification/Clustering\n",
    "Feed to classification/clustering algorithm to make some predictions, like what kind of data this is, or which category this news article belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
