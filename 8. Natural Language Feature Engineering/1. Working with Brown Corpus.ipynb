{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Pipeline \n",
    "This pipeline is used to create features for our textual data\n",
    "1. Data Collection\n",
    "2. Tokenization, stopword removal, stemming\n",
    "3. Building a common vocabulary\n",
    "4. Vectorizing the documents\n",
    "5. Performing classification/clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "# https://www.nltk.org/book/ch02.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.corpus.util.LazyCorpusLoader'>\n"
     ]
    }
   ],
   "source": [
    "print(type(brown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CategorizedTaggedCorpusReader in '/Users/riagupta/nltk_data/corpora/brown'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown?\n",
    "\n",
    "# A reader for part-of-speech tagged corpora whose documents are divided into categories \n",
    "# based on their file identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = brown.sents(categories='fiction')\n",
    "# Returns a list of sentences, each encoded as a list of word strings. We can specify category (or leave blank for all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4249"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "\n",
    "# ie, 4249 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Thirty-three'], ['Scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments', '.']\n"
     ]
    }
   ],
   "source": [
    "print(data[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization, Stopword Removal and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Tokenization\n",
    "Converting each sentence into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a very pleasant day, the weather was cool and there were light showers. I went to the market to buy some fruits.\n"
     ]
    }
   ],
   "source": [
    "text = \"It was a very pleasant day, the weather was cool and there were light showers. I went to the market to buy some fruits.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['It was a very pleasant day, the weather was cool and there were light showers.', 'I went to the market to buy some fruits.']\n"
     ]
    }
   ],
   "source": [
    "# To split our text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(type(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['it', 'was', 'a', 'very', 'pleasant', 'day', ',', 'the', 'weather', 'was', 'cool', 'and', 'there', 'were', 'light', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "# To split our text into words\n",
    "words = word_tokenize(sentences[0].lower())\n",
    "print(type(words))\n",
    "print(words)\n",
    "# Convert to lower case to perform proper stopword removal later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing using Regular Expressions\n",
    "Word tokenizer cant handle complex tokenizations. So we use RegExp Tokenizer class in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send all the 50 documents related to clauses 1,2,3 at abc@gmail.com']\n",
      "['send', 'all', 'the', '50', 'documents', 'related', 'to', 'clauses', '1,2,3', 'at', 'abc', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Send all the 50 documents related to clauses 1,2,3 at abc@gmail.com\"\n",
    "\n",
    "sents = sent_tokenize(text2)\n",
    "print(sents)\n",
    "\n",
    "word_list = word_tokenize(sents[0].lower())\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z@]+\")\n",
    "# Pass a regular expression pattern to remove whatever you want to remove\n",
    "# This regular expression allows chars a-z, A-Z, and spcl char @ => So, it only removes numbers\n",
    "# Create a regular expression using cheat sheet on regexpal.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Send all the 50 documents related to clauses 1,2,3 at abc@gmail.com\n",
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'clauses', 'at', 'abc@gmail', 'com']\n"
     ]
    }
   ],
   "source": [
    "print(text2)\n",
    "print(tokenizer.tokenize(text2))     # With numbers removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Stopword Removal\n",
    "Removing unimportant words like is, an, the, it, there, and, has, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(sw))\n",
    "print(type(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(text):\n",
    "    useful_words = [w for w in text if w not in sw]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pleasant', 'day', ',', 'weather', 'cool', 'light', 'showers', '.']\n"
     ]
    }
   ],
   "source": [
    "# Filtering words from our sentence using list comprehension\n",
    "\n",
    "useful_words = [w for w in words if w not in sw]\n",
    "print(useful_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Stemming\n",
    "- A process that transforms words to (verbs, plurals) to their radical form\n",
    "- Reducing words to their root words.\n",
    "- Purpose: Preserve the semantics of the sentence without increasing the number of unique tokens\n",
    "\n",
    "eg. \n",
    "- Cat jumps\n",
    "- Cat jumped\n",
    "- Cat is jumping \n",
    "\n",
    "All can be reduced to jump <br>\n",
    "This can also be done through lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foxes', 'love', 'to', 'make', 'jumps', 'the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lovely', 'dog', 'from', 'a', 'feet', 'high', 'wall']\n"
     ]
    }
   ],
   "source": [
    "text = \"Foxes love to make jumps. The quick brown fox was seen jumping over the lovely dog from a 6 feet high wall\"\n",
    "\n",
    "words_list = tokenizer.tokenize(text.lower())\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foxes', 'love', 'make', 'jumps', 'quick', 'brown', 'fox', 'seen', 'jumping', 'lovely', 'dog', 'feet', 'high', 'wall']\n"
     ]
    }
   ],
   "source": [
    "words_list = filter_words(words_list)\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Stemmers:\n",
    "1. Snowball Stemmer (Multilingual => supports French, German, Russian, etc)\n",
    "2. Porter Stemmer\n",
    "3. Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jump\n",
      "jump\n",
      "jumper\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('jumped'))\n",
    "print(ps.stem('jumps'))\n",
    "print(ps.stem('jumping'))\n",
    "print(ps.stem('jumper'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "awesom\n",
      "teeth\n",
      "teenag\n",
      "quickli\n"
     ]
    }
   ],
   "source": [
    "print(ps.stem('lovely'))\n",
    "print(ps.stem('awesome'))\n",
    "print(ps.stem('teeth'))\n",
    "print(ps.stem('teenager'))\n",
    "print(ps.stem('quickly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jump\n",
      "jump\n",
      "jump\n",
      "lov\n",
      "awesom\n",
      "tee\n",
      "teen\n"
     ]
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "\n",
    "print(ls.stem('jumped'))\n",
    "print(ls.stem('jumps'))\n",
    "print(ls.stem('jumping'))\n",
    "print(ls.stem('jumper'))\n",
    "print(ls.stem('lovely'))\n",
    "print(ls.stem('awesome'))\n",
    "print(ls.stem('teeth'))\n",
    "print(ls.stem('teenager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump\n",
      "jump\n",
      "jump\n",
      "jumper\n",
      "love\n",
      "awesom\n",
      "teeth\n",
      "teenag\n"
     ]
    }
   ],
   "source": [
    "ss = SnowballStemmer('english')\n",
    "print(ss.stem('jumped'))\n",
    "print(ss.stem('jumps'))\n",
    "print(ss.stem('jumping'))\n",
    "print(ss.stem('jumper'))\n",
    "print(ss.stem('lovely'))\n",
    "print(ss.stem('awesome'))\n",
    "print(ss.stem('teeth'))\n",
    "print(ss.stem('teenager'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parl\n",
      "parl\n",
      "parl\n",
      "parl\n",
      "parlon\n",
      "fin\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "ss2 = SnowballStemmer('french')\n",
    "\n",
    "print(ss2.stem(\"parlais\"))\n",
    "print(ss2.stem(\"parles\"))\n",
    "print(ss2.stem(\"parler\"))\n",
    "print(ss2.stem(\"parlerai\"))\n",
    "print(ss2.stem(\"parlons\"))\n",
    "print(ss2.stem(\"finir\"))\n",
    "print(ss2.stem(\"finit\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to perform all 3 steps: Tokenization, stopword removal, and stemming, and also removes any leading\n",
    "# or trailing white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He had nothing much to say to her but that he said anything seemed to please her and he accompanied her on some of her unusually searching tours of Tokyo .\n"
     ]
    }
   ],
   "source": [
    "text = brown.sents(categories='romance')\n",
    "sent = ' '.join(text[3743])\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           He had nothing much to say to her but that          he said anything seemed to please her and he accompanied her on some of her unusually searching tours of Tokyo.             \n",
      "\n",
      "['he', 'had', 'nothing', 'much', 'to', 'say', 'to', 'her', 'but', 'that', 'he', 'said', 'anything', 'seemed', 'to', 'please', 'her', 'and', 'he', 'accompanied', 'her', 'on', 'some', 'of', 'her', 'unusually', 'searching', 'tours', 'of', 'tokyo']\n",
      "['nothing', 'much', 'say', 'said', 'anything', 'seemed', 'please', 'accompanied', 'unusually', 'searching', 'tours', 'tokyo']\n",
      "\n",
      "['nothing', 'much', 'say', 'said', 'anything', 'seemed', 'please', 'accompanied', 'unusually', 'searching', 'tours', 'tokyo']\n",
      "\n",
      "['noth', 'much', 'say', 'said', 'anyth', 'seem', 'pleas', 'accompani', 'unusu', 'search', 'tour', 'tokyo']\n",
      "\n",
      "['noth', 'much', 'say', 'said', 'anyth', 'seem', 'pleas', 'accompani', 'unusu', 'search', 'tour', 'tokyo']\n",
      "\n",
      "['noth', 'much', 'say', 'said', 'anyth', 'seem', 'pleas', 'accompany', 'unus', 'search', 'tour', 'tokyo']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cleanup(sent):\n",
    "    print(sent+'\\n')\n",
    "    tokenizer = RegexpTokenizer(\"[a-zA-Z]+\")\n",
    "    sent = tokenizer.tokenize(sent.lower())\n",
    "    print(sent)\n",
    "    \n",
    "    sw = stopwords.words('english')\n",
    "    sent = [w for w in sent if w not in sw]\n",
    "    print(sent)\n",
    "    \n",
    "    ss = SnowballStemmer('english')\n",
    "    ps = PorterStemmer()\n",
    "    ls = LancasterStemmer()\n",
    "    stemmed_words = []\n",
    "    stemmed_words2 = []\n",
    "    stemmed_words3 = []\n",
    "\n",
    "    for w in sent:\n",
    "        stemmed_words.append(ss.stem(w))\n",
    "        stemmed_words2.append(ps.stem(w))\n",
    "        stemmed_words3.append(ls.stem(w))\n",
    "\n",
    "    print()\n",
    "    print(sent)\n",
    "    print()\n",
    "    print(stemmed_words)\n",
    "    print()\n",
    "    print(stemmed_words2)\n",
    "    print()\n",
    "    print(stemmed_words3)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "text = \"           He had nothing much to say to her but that          he said anything seemed to please her and he accompanied her on some of her unusually searching tours of Tokyo.             \"\n",
    "   \n",
    "cleanup(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Lemmatization (Try it yourself)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "l = WordNetLemmatizer()\n",
    "l.lemmatize(\"crying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building Common Vocabulary\n",
    "After extracting all the important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "vectorized_corpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 42)\n",
      "[[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1\n",
      "  0 2 0 1 0 2]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      "  1 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 3 0 0 0\n",
      "  0 0 0 0 1 0]\n",
      " [1 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 1 0\n",
      "  0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus.shape)\n",
    "print(vectorized_corpus)\n",
    "\n",
    "# Each vector entry denotes the frequency of the word at that index in the dictionary. Indices are described in random\n",
    "# order and can be checked in next cell using get_feature_names() function\n",
    "\n",
    "# Eg. word \"world\" is at last index in dictionary and appears 2x in sentence 1 => array has value 2 at first row \n",
    "# last index\n",
    "# Eg. word \"the\" is at 10th last index in dictionary and appears 3x in sentence 3 => array has value 3 at third row \n",
    "# 10th last index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'at', 'based', 'be', 'capt', 'confident', 'cricket', 'cup', 'elections', 'exciting', 'hearts', 'held', 'indian', 'is', 'kohli', 'lanka', 'laurate', 'lok', 'movie', 'next', 'nobel', 'of', 'people', 'pm', 'raazi', 'real', 'sabha', 'says', 'spy', 'sri', 'story', 'team', 'the', 'thriller', 'upon', 'virat', 'we', 'will', 'win', 'wins', 'won', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'wins': 39, 'world': 41, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'win': 38, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 40, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary which maps word to index\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "[array(['an', 'at', 'based', 'cup', 'elections', 'exciting', 'hearts',\n",
      "       'held', 'indian', 'is', 'kohli', 'lanka', 'laurate', 'lok',\n",
      "       'movie', 'next', 'nobel', 'of', 'people', 'pm', 'raazi', 'real',\n",
      "       'sabha', 'says', 'spy', 'sri', 'story', 'team', 'the', 'thriller',\n",
      "       'upon', 'virat', 'we', 'will', 'win', 'wins', 'won', 'world'],\n",
      "      dtype='<U9')]\n",
      "[array(['an', 'at', 'based', 'be', 'capt', 'confident', 'cricket', 'cup',\n",
      "       'elections', 'exciting', 'hearts', 'held', 'indian', 'is', 'kohli',\n",
      "       'lanka', 'laurate', 'lok', 'movie', 'next', 'nobel', 'of',\n",
      "       'people', 'pm', 'raazi', 'real', 'sabha', 'says', 'spy', 'sri',\n",
      "       'story', 'team', 'the', 'thriller', 'upon', 'virat', 'we', 'will',\n",
      "       'win', 'wins', 'won', 'world'], dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "# Reverse mapping: Given any self-created vector, we want to see what sentence is created\n",
    "import numpy as np\n",
    "\n",
    "vec = np.ones((42,))\n",
    "vec[3:7] = 0\n",
    "print(vec)\n",
    "print()\n",
    "\n",
    "print(cv.inverse_transform(vec))\n",
    "vec[3:7] = 1\n",
    "\n",
    "print(cv.inverse_transform(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Check index of any word\n",
    "print(cv.vocabulary_['virat'])\n",
    "print(cv.vocabulary_['kohli'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vectorize the Document\n",
    "Use the bag-of-words model to convert this vocabulary to vectors. \n",
    "\n",
    "eg. It was raining and the cat jumped <br>\n",
    "Vocab => rain, cat, jump\n",
    "\n",
    "For sentence: The cat was jumping, vector is [0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indian',\n",
       " 'cricket',\n",
       " 'team',\n",
       " 'wins',\n",
       " 'world',\n",
       " 'cup',\n",
       " 'says',\n",
       " 'capt',\n",
       " 'virat',\n",
       " 'kohli',\n",
       " 'world',\n",
       " 'cup',\n",
       " 'held',\n",
       " 'sri',\n",
       " 'lanka']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For a corpus with one million words, each sentence vector is represented as a vector of size 1 million, so,\n",
    "# Effectively reduce the size of the vector\n",
    "\n",
    "def myTokenizer(sentence):\n",
    "    words = tokenizer.tokenize(sentence.lower())\n",
    "    return filter_words(words)\n",
    "\n",
    "myTokenizer(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 2]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0]]\n",
      "33\n",
      "[array(['capt', 'cricket', 'cup', 'held', 'indian', 'kohli', 'lanka',\n",
      "       'says', 'sri', 'team', 'virat', 'wins', 'world'], dtype='<U9')]\n",
      "[array(['based', 'capt', 'cricket', 'cup', 'held', 'indian', 'kohli',\n",
      "       'lanka', 'says', 'sri', 'team', 'virat', 'wins', 'world'],\n",
      "      dtype='<U9')]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "print(vectorized_corpus.toarray())\n",
    "print(len(vectorized_corpus.toarray()[0]))\n",
    "\n",
    "vc = vectorized_corpus[0].toarray()\n",
    "\n",
    "print(cv.inverse_transform(vc[0]))      # Feed array as input, not vectorized_corpus directly\n",
    "# print(vc[0])\n",
    "vc[0][0] = 1\n",
    "print(cv.inverse_transform(vc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram, Bigram and N-gram Models\n",
    "The current type of model is called Unigram Bag of Words model, where we only see the frequency of one word\n",
    "\n",
    "Sometimes, it is useful to see more than one words to get more context. So, we use n-gram model, where we see n words surrounding a particular word.<br>\n",
    "- eg. I don't like ice-cream <br>\n",
    "    - Using a unigram model may not convey the actual context if don't and like are not considered together, ie, the disliking behaviour will be missed.<br>\n",
    "    - Using a bigram model (pairs of 2) will capture the negative sentiment associated with \"don't like\"  as they will be considered together\n",
    "\n",
    "***Bigram Model***: We make a pair of every 2 consecutive words in a sentence and store them as a feature <br>\n",
    "In our current model, we keep only one word as a feature, but in n-gram model, we store n consecutive words as one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 9, 'cricket': 3, 'team': 26, 'wins': 31, 'world': 32, 'cup': 4, 'says': 22, 'capt': 1, 'virat': 29, 'kohli': 10, 'held': 8, 'sri': 24, 'lanka': 11, 'win': 30, 'next': 15, 'lok': 13, 'sabha': 21, 'elections': 5, 'confident': 2, 'pm': 18, 'nobel': 16, 'laurate': 12, 'hearts': 7, 'people': 17, 'movie': 14, 'raazi': 19, 'exciting': 6, 'spy': 23, 'thriller': 27, 'based': 0, 'upon': 28, 'real': 20, 'story': 25}\n",
      "33\n",
      "[[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 2]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer, ngram_range=(1,1))    # Only unigrams\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vc = vectorized_corpus.toarray()\n",
    "print(cv.vocabulary_)\n",
    "print(len(vc[0]))\n",
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian cricket': 10, 'cricket team': 3, 'team wins': 26, 'wins world': 31, 'world cup': 32, 'cup says': 5, 'says capt': 22, 'capt virat': 1, 'virat kohli': 29, 'kohli world': 13, 'cup held': 4, 'held sri': 9, 'sri lanka': 25, 'win next': 30, 'next lok': 17, 'lok sabha': 15, 'sabha elections': 21, 'elections says': 6, 'says confident': 23, 'confident indian': 2, 'indian pm': 11, 'nobel laurate': 18, 'laurate hearts': 14, 'hearts people': 8, 'movie raazi': 16, 'raazi exciting': 19, 'exciting indian': 7, 'indian spy': 12, 'spy thriller': 24, 'thriller based': 27, 'based upon': 0, 'upon real': 28, 'real story': 20}\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer, ngram_range=(2,2))    # Only bigrams\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vc = vectorized_corpus.toarray()\n",
    "print(cv.vocabulary_)\n",
    "print(len(vc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 19, 'cricket': 6, 'team': 52, 'wins': 62, 'world': 64, 'cup': 8, 'says': 44, 'capt': 2, 'virat': 58, 'kohli': 23, 'held': 17, 'sri': 49, 'lanka': 25, 'indian cricket': 20, 'cricket team': 7, 'team wins': 53, 'wins world': 63, 'world cup': 65, 'cup says': 10, 'says capt': 45, 'capt virat': 3, 'virat kohli': 59, 'kohli world': 24, 'cup held': 9, 'held sri': 18, 'sri lanka': 50, 'win': 60, 'next': 32, 'lok': 28, 'sabha': 42, 'elections': 11, 'confident': 4, 'pm': 37, 'win next': 61, 'next lok': 33, 'lok sabha': 29, 'sabha elections': 43, 'elections says': 12, 'says confident': 46, 'confident indian': 5, 'indian pm': 21, 'nobel': 34, 'laurate': 26, 'hearts': 15, 'people': 36, 'nobel laurate': 35, 'laurate hearts': 27, 'hearts people': 16, 'movie': 30, 'raazi': 38, 'exciting': 13, 'spy': 47, 'thriller': 54, 'based': 0, 'upon': 56, 'real': 40, 'story': 51, 'movie raazi': 31, 'raazi exciting': 39, 'exciting indian': 14, 'indian spy': 22, 'spy thriller': 48, 'thriller based': 55, 'based upon': 1, 'upon real': 57, 'real story': 41}\n",
      "66\n",
      "[[0 0 1 1 0 0 1 1 2 1 1 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 1 2 2]\n",
      " [0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0\n",
      "  0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      "  0 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer, ngram_range=(1,2))    # Mix of unigrams and bigrams\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vc = vectorized_corpus.toarray()\n",
    "print(cv.vocabulary_)\n",
    "print(len(vc[0]))\n",
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 28, 'cricket': 9, 'team': 74, 'wins': 89, 'world': 92, 'cup': 12, 'says': 63, 'capt': 3, 'virat': 83, 'kohli': 34, 'held': 25, 'sri': 71, 'lanka': 37, 'indian cricket': 29, 'cricket team': 10, 'team wins': 75, 'wins world': 90, 'world cup': 93, 'cup says': 15, 'says capt': 64, 'capt virat': 4, 'virat kohli': 84, 'kohli world': 35, 'cup held': 13, 'held sri': 26, 'sri lanka': 72, 'indian cricket team': 30, 'cricket team wins': 11, 'team wins world': 76, 'wins world cup': 91, 'world cup says': 95, 'cup says capt': 16, 'says capt virat': 65, 'capt virat kohli': 5, 'virat kohli world': 85, 'kohli world cup': 36, 'world cup held': 94, 'cup held sri': 14, 'held sri lanka': 27, 'win': 86, 'next': 47, 'lok': 41, 'sabha': 60, 'elections': 17, 'confident': 6, 'pm': 54, 'win next': 87, 'next lok': 48, 'lok sabha': 42, 'sabha elections': 61, 'elections says': 18, 'says confident': 66, 'confident indian': 7, 'indian pm': 31, 'win next lok': 88, 'next lok sabha': 49, 'lok sabha elections': 43, 'sabha elections says': 62, 'elections says confident': 19, 'says confident indian': 67, 'confident indian pm': 8, 'nobel': 50, 'laurate': 38, 'hearts': 23, 'people': 53, 'nobel laurate': 51, 'laurate hearts': 39, 'hearts people': 24, 'nobel laurate hearts': 52, 'laurate hearts people': 40, 'movie': 44, 'raazi': 55, 'exciting': 20, 'spy': 68, 'thriller': 77, 'based': 0, 'upon': 80, 'real': 58, 'story': 73, 'movie raazi': 45, 'raazi exciting': 56, 'exciting indian': 21, 'indian spy': 32, 'spy thriller': 69, 'thriller based': 78, 'based upon': 1, 'upon real': 81, 'real story': 59, 'movie raazi exciting': 46, 'raazi exciting indian': 57, 'exciting indian spy': 22, 'indian spy thriller': 33, 'spy thriller based': 70, 'thriller based upon': 79, 'based upon real': 2, 'upon real story': 82}\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer, ngram_range=(1,3))    # Mix of unigrams, bigrams, and trigrams\n",
    "vectorized_corpus = cv.fit_transform(corpus)\n",
    "vc = vectorized_corpus.toarray()\n",
    "print(cv.vocabulary_)\n",
    "print(len(vc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Normalization\n",
    "- Avoid features which occur very often, as they contain less information (regarding classification, clustering, etc.)\n",
    "- Information decreases as the number of occurrences increase across different type of documents\n",
    "    - eg. For news classification, if we have 4 categories (Sports, Politics, Society, Movies) and they contain term 'Indian' as (Indian Cricket Team, Indian PM, -, Indian spy thriller), the term Indian doesn't convey much information about the type of the news\n",
    "    - So it should have less importance in classification\n",
    "    - Give more weight to terms (words) which are more specific to given category\n",
    "    \n",
    "- So, we define Term Frequency (TF) and Inverse Document Frequency (IDF) to assign weight to every term\n",
    "\n",
    "\n",
    "Term Frequency, $t_f (t, d)$ = Number of times term t occurs in the document d\n",
    "\n",
    "Inverse Document Frequency, $ idf (t, d)$ = $ log \\frac{n}{1 + count(d, t)}$, where count(d, t) is number of documents term t appears in, and n is total number of documents<br>\n",
    "If count(d, t) is high, so 1+count(d, t) is high, so overall fraction is small and idf is low, so it has less importance if it is present in multiple documents\n",
    "\n",
    "\n",
    "Weight = tf * idf\n",
    "\n",
    "\n",
    "**TF, Term Frequency:** which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "\n",
    "**IDF, Inverse Document Frequency:** which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "IDF(t) = $log_e$(Total number of documents / Number of documents with term t in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.17142549 0.17142549 0.         0.\n",
      "  0.17142549 0.17142549 0.34285097 0.17142549 0.17142549 0.\n",
      "  0.         0.         0.         0.         0.         0.17142549\n",
      "  0.17142549 0.10941867 0.17142549 0.         0.         0.17142549\n",
      "  0.17142549 0.17142549 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13515382 0.17142549 0.         0.\n",
      "  0.         0.17142549 0.17142549 0.         0.17142549 0.17142549\n",
      "  0.         0.         0.         0.         0.17142549 0.17142549\n",
      "  0.         0.         0.17142549 0.17142549 0.34285097 0.34285097]\n",
      " [0.         0.         0.         0.         0.24977372 0.24977372\n",
      "  0.         0.         0.         0.         0.         0.24977372\n",
      "  0.24977372 0.         0.         0.         0.         0.\n",
      "  0.         0.15942733 0.         0.24977372 0.         0.\n",
      "  0.         0.         0.         0.         0.24977372 0.24977372\n",
      "  0.         0.         0.24977372 0.24977372 0.         0.\n",
      "  0.         0.24977372 0.         0.         0.         0.\n",
      "  0.24977372 0.24977372 0.19692447 0.         0.24977372 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.24977372 0.24977372 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37796447 0.37796447 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37796447 0.37796447 0.         0.\n",
      "  0.         0.         0.         0.         0.37796447 0.37796447\n",
      "  0.37796447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.23307927 0.23307927 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.23307927 0.23307927 0.         0.         0.\n",
      "  0.         0.14877148 0.         0.         0.23307927 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.23307927 0.23307927 0.         0.         0.         0.\n",
      "  0.         0.         0.23307927 0.23307927 0.23307927 0.23307927\n",
      "  0.         0.         0.         0.         0.         0.23307927\n",
      "  0.23307927 0.         0.         0.23307927 0.         0.\n",
      "  0.23307927 0.23307927 0.23307927 0.23307927 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidfvectorizer = TfidfVectorizer(tokenizer=myTokenizer, ngram_range=(1,2), norm='l2') \n",
    "vectorized_corpus = tfidfvectorizer.fit_transform(corpus).toarray()\n",
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indian': 19, 'cricket': 6, 'team': 52, 'wins': 62, 'world': 64, 'cup': 8, 'says': 44, 'capt': 2, 'virat': 58, 'kohli': 23, 'held': 17, 'sri': 49, 'lanka': 25, 'indian cricket': 20, 'cricket team': 7, 'team wins': 53, 'wins world': 63, 'world cup': 65, 'cup says': 10, 'says capt': 45, 'capt virat': 3, 'virat kohli': 59, 'kohli world': 24, 'cup held': 9, 'held sri': 18, 'sri lanka': 50, 'win': 60, 'next': 32, 'lok': 28, 'sabha': 42, 'elections': 11, 'confident': 4, 'pm': 37, 'win next': 61, 'next lok': 33, 'lok sabha': 29, 'sabha elections': 43, 'elections says': 12, 'says confident': 46, 'confident indian': 5, 'indian pm': 21, 'nobel': 34, 'laurate': 26, 'hearts': 15, 'people': 36, 'nobel laurate': 35, 'laurate hearts': 27, 'hearts people': 16, 'movie': 30, 'raazi': 38, 'exciting': 13, 'spy': 47, 'thriller': 54, 'based': 0, 'upon': 56, 'real': 40, 'story': 51, 'movie raazi': 31, 'raazi exciting': 39, 'exciting indian': 14, 'indian spy': 22, 'spy thriller': 48, 'thriller based': 55, 'based upon': 1, 'upon real': 57, 'real story': 41}\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "print(tfidfvectorizer.vocabulary_)\n",
    "print(len(tfidfvectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Classification/Clustering\n",
    "Feed to classification/clustering algorithm to make some predictions, like what kind of data this is, or which category this news article belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!', 'Hey', 'Coding', 'Welcome', 'to', 'Blocks', '.', '?'}\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "sent = \"Hey! Welcome to Coding Blocks ?.\"\n",
    "words = set(word_tokenize(sent))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
